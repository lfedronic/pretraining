{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IqM-T1RTzY6C"
   },
   "source": [
    "### Text Completion / Raw Text Training\n",
    "This is a community notebook collaboration with [Mithex].\n",
    "\n",
    "We train on `Tiny Stories` (link [here](https://huggingface.co/datasets/roneneldan/TinyStories)) which is a collection of small stories. For example:\n",
    "```\n",
    "Once upon a time, there was a little car named Beep. Beep loved to go fast and play in the sun.\n",
    "Beep was a healthy car because he always had good fuel....\n",
    "```\n",
    "Instead of `Alpaca`'s Question Answer format, one only needs 1 column - the `\"text\"` column. This means you can finetune on any dataset and let your model act as a text completion model, like for novel writing.\n",
    "\n",
    "---\n",
    "\n",
    "To run this, press \"Runtime\" and press \"Run all\" on a **free** Tesla T4 Google Colab instance!\n",
    "<div class=\"align-center\">\n",
    "  <a href=\"https://github.com/unslothai/unsloth\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/unsloth%20new%20logo.png\" width=\"115\"></a>\n",
    "  <a href=\"https://discord.gg/u54VK8m8tk\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/Discord.png\" width=\"145\"></a>\n",
    "  <a href=\"https://ko-fi.com/unsloth\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/Kofi button.png\" width=\"145\"></a></a> Join Discord if you need help + ‚≠ê <i>Star us on <a href=\"https://github.com/unslothai/unsloth\">Github</a> </i> ‚≠ê\n",
    "</div>\n",
    "\n",
    "To install Unsloth on your own computer, follow the installation instructions on our Github page [here](https://github.com/unslothai/unsloth#installation-instructions---conda).\n",
    "\n",
    "You will learn how to do [data prep](#Data), how to [train](#Train), how to [run the model](#Inference), & [how to save it](#Save) (eg for Llama.cpp)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r2v_X2fA0Df5"
   },
   "source": [
    "* We support Llama, Mistral, CodeLlama, TinyLlama, Vicuna, Open Hermes etc\n",
    "* And Yi, Qwen, Deepseek, all Llama, Mistral derived archs.\n",
    "* We support 16bit LoRA or 4bit QLoRA. Both 2x faster.\n",
    "* `max_seq_length` can be set to anything, since we do automatic RoPE Scaling via [kaiokendev's](https://kaiokendev.github.io/til) method.\n",
    "* [**NEW**] We make Llama-3 15 trillion tokens **2x faster**! See our [Llama-3 notebook](https://colab.research.google.com/drive/135ced7oHytdxu3N2DNe1Z0kqjyYIkDXp?usp=sharing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 316,
     "referenced_widgets": [
      "e493a78dc35b4e989bcf8412efd992f1",
      "286282f0f5f14ed7b5516a5c86a14717",
      "0290aa5b6f2148c28d5db378da4eaa9d",
      "5f216daf476d4ffaabccdc97deab3802",
      "d328c3949a9e4092883d711d7fb9d945",
      "5f7b89d1f7bd4c04a1be54d62db1de90",
      "3d385dbd0f9742cbba14a00e00238c7a",
      "000f8c3e543e474b906ae3ed88e3029b",
      "b07aa794ee6246ac89b923fa3bfbc4f3",
      "4f8e75200153430892f20a2e0d9683ad",
      "b7c8be1947f948dcbe988f11e2a47280",
      "e8aec73257844a19bcb404ea22a09d2c",
      "d94e0126736f4f7bbdc350f2b25c57a8",
      "b4a9e32a271a46438e9c188d311183c3",
      "38b260d5f1604e3ab8303bdc7bed09a2",
      "00e8bc9a30dd46d0ab5bd64d8cb603c9",
      "0e6a85d4230b4d868eab609e3bbcb26b",
      "998156a97ea54b47a144feb168b422f3",
      "5c0aef5eaef84c6a885185551a99f489",
      "593b56fd26cc4a91948934b9881d37c3",
      "0b4bb49b21094acd9cf0f698a1b5dc9f",
      "78c4e03ba858481eb017abbad634e563",
      "7839c152c70f4333b6fd56a0de422ed8",
      "ac531852a2e0473e8af8ba9962fe4b91",
      "f88f3dc016c94bbc81f9c296693f24dd",
      "8be71e7b5e914b0e8a22eddef64a4a94",
      "9ce88525df584216aace0b168e4a4798",
      "7aba3c7d80b64710a8861f6a065daade",
      "5b7e9400c0584cb3b8556ced0f9fb024",
      "9c86fec49106407e8cdb0728bf8283d5",
      "e92d928ff17e466fbf0b3934f987cdaa",
      "acfb3d307dd24dac957e4fe9efcf0604",
      "5c88f1449b204e3ca33c7c0fdca5a2e3",
      "7d11bfead4744ce7abff3ee28aebd9c5",
      "db64af76fabf4e4686e0e70a668bc112",
      "45ef29a0d84641e5a34eca5d97f285b9",
      "c63635c3ee9d42bb8b5214ceb05f58f6",
      "738bc2d947da4c9f93eaec6dc10865ea",
      "5fb7c2fe970543a2affb765ef1e489e3",
      "4a4ebf72168c487bbe2138ad295211e2",
      "6d6ab6642f074af7ad3c37e10dd7de12",
      "213242e14f81470a943f3d3608223907",
      "a9e959ffd8f242b0be7cd48285aee3f8",
      "0b235b37c13e4c8197c8fd149ed49a28",
      "fdbe574155984ae689e858c45ce529cf",
      "e0018988debb4ccabcbdae7b2f2fe273",
      "014eb62c344349119bfdd746d235719e",
      "13628eb9c7864898a4c95b614fc8f3cb",
      "a9c82ef6d857430ca0ed5a8d06afc5b9",
      "5206db6138cb4d3486729940f430193b",
      "75c6f712315f4d448506cc642f852bad",
      "2f63f930dd9046a1a6ca4e161091996d",
      "3b150c7d44a845ebad1c1f28cef3ac26",
      "20a318008c5f4dd083b0cedbd347d604",
      "5019072f0df344828bf62842e0d70aae",
      "7a6443281ed24a2a8d34d0460077e02a",
      "12a3eb9a7ad840c1a04b389fb848855b",
      "b8183970a8114c85b0f292b2c6c9e123",
      "c88ef51daaf0412fac372cbb5b722335",
      "e93e414082b64fffa09df3091dce6f72",
      "4e804623e6ac44d099a7aa0de89aa300",
      "f24328d31e3d48d0bd76785140125a28",
      "693260784e824a90b2931c520749963a",
      "e3bd04fcfd734ac8b6ee5af8b0d1131b",
      "200d56e0e3e94909bba8ca04777d1769",
      "9a08155d3e164f4abedbea66efda5d76"
     ]
    },
    "id": "QmUBVEnvCDJv",
    "outputId": "28750a0d-17c4-4c7f-9885-3d01d821c8c4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ü¶• Unsloth Zoo will now patch everything to make training faster!\n",
      "Unsloth: We'll be using `/tmp/unsloth_compiled_cache` for temporary Unsloth patches.\n",
      "Standard import failed for UnslothPRMTrainer: No module named 'UnslothPRMTrainer'. Using tempfile instead!\n",
      "==((====))==  Unsloth 2025.3.10: Fast Llama patching. Transformers: 4.49.0.\n",
      "   \\\\   /|    NVIDIA L40S. Num GPUs = 1. Max memory: 44.521 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 8.9. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post3. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "789d8576dcd7474e8094944b24307c2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/331k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a09ce77cac5d4a57afe0b91d477980d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "651400ee3c1b4e50b852a26fb4ec3d1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00006.safetensors:   0%|          | 0.00/7.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4ff048f55044c21917e4b36d681a774",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00006.safetensors:   0%|          | 0.00/6.90G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95f16e6ae8074ee2bbf9b08876777fc5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00006.safetensors:   0%|          | 0.00/6.94G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65fdf926799e4d5392776ec74cc5dee9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00006.safetensors:   0%|          | 0.00/6.94G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f85c0ae9437042a2a594a5e365d2cebb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00005-of-00006.safetensors:   0%|          | 0.00/6.99G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec5a6810ab514a24a3fba3e5badb86c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00006-of-00006.safetensors:   0%|          | 0.00/4.75G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7dd80b1a656a4ce38ea72bb83c343f16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b54b469ae084cd98c7e0366e8d54399",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/198 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f07048d712c4e6d8b128af1486bc2f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/50.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4ea480666e2469dbcf3ada7ddc29d72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "728ee5a53b854045a3d0b0a8b53ab3c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "# 4bit pre quantized models we support for 4x faster downloading + no OOMs.\n",
    "fourbit_models = [\n",
    "    \"unsloth/mistral-7b-v0.3-bnb-4bit\",      # New Mistral v3 2x faster!\n",
    "    \"unsloth/mistral-7b-instruct-v0.3-bnb-4bit\",\n",
    "    \"unsloth/llama-3-8b-bnb-4bit\",           # Llama-3 15 trillion tokens model 2x faster!\n",
    "    \"unsloth/llama-3-8b-Instruct-bnb-4bit\",\n",
    "    \"unsloth/llama-3-70b-bnb-4bit\",\n",
    "    \"unsloth/Phi-3-mini-4k-instruct\",        # Phi-3 2x faster!\n",
    "    \"unsloth/Phi-3-medium-4k-instruct\",\n",
    "    \"unsloth/mistral-7b-bnb-4bit\",\n",
    "    \"unsloth/gemma-7b-bnb-4bit\",             # Gemma 2.2x faster!\n",
    "] # More models at https://huggingface.co/unsloth\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/llama-3-70b-bnb-4bit\", # \"unsloth/mistral-7b\" for 16bit loading\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "biRhv5Caf5SK"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SXd9bTZd1aaL"
   },
   "source": [
    "We now add LoRA adapters so we only need to update 1 to 10% of all parameters!\n",
    "\n",
    "We also add `embed_tokens` and `lm_head` to allow the model to learn out of distribution data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6bZsfBuZDeCL",
    "outputId": "b498c292-71a6-42e8-8b0b-4b43b74562b4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Offloading input_embeddings to disk to save VRAM\n",
      "Unsloth: Offloading output_embeddings to disk to save VRAM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2025.3.10 patched 80 layers with 80 QKV layers, 80 O layers and 80 MLP layers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Training embed_tokens in mixed precision to save VRAM\n",
      "Unsloth: Training lm_head in mixed precision to save VRAM\n"
     ]
    }
   ],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 128, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",\n",
    "\n",
    "                      \"embed_tokens\", \"lm_head\",], # Add for continual pretraining\n",
    "    lora_alpha = 32,\n",
    "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
    "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
    "    random_state = 3407,\n",
    "    use_rslora = True,  # We support rank stabilized LoRA\n",
    "    loftq_config = None, # And LoftQ\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vITh0KVJ10qX"
   },
   "source": [
    "<a name=\"Data\"></a>\n",
    "### Data Prep\n",
    "We now use the Tiny Stories dataset from https://huggingface.co/datasets/roneneldan/TinyStories. We only sample the first 5000 rows to speed training up. We must add `EOS_TOKEN` or `tokenizer.eos_token` or else the model's generation will go on forever.\n",
    "\n",
    "If you want to use the `ChatML` template for ShareGPT datasets, try our conversational [notebook](https://colab.research.google.com/drive/1Aau3lgPzeZKQ-98h69CCu1UJcvIBLmy2?usp=sharing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "yXt8Na97yRe7"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f8497e3e4f1413e850e8fa9ef28f8d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/272 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ba46c47f55743b7a240aeb3ccd2a1c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00000-of-00001.parquet:   0%|          | 0.00/333k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d00f564aa724aa9bd6df6e71a37720a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/158 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0b6a3f110a142548efd999edc1294b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/158 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"lfedronic/llama-factory-dataset\")\n",
    "EOS_TOKEN = tokenizer.eos_token\n",
    "def formatting_prompts_func(examples):\n",
    "    return { \"text\" : [example + EOS_TOKEN for example in examples[\"text\"]] }\n",
    "dataset = dataset.map(formatting_prompts_func, batched = True,)['train']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8ENCwwLaWjud"
   },
   "source": [
    "Print out 5 stories from `Tiny Stories`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "EMv7nqxz6fta",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "353a5c01-0a17-49f4-8513-7b99f69be7af"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================\n",
      "FILE NAME: data/belle_multiturn/belle_multiturn.py\n",
      "\n",
      "\n",
      "import json\n",
      "import os\n",
      "\n",
      "import datasets\n",
      "\n",
      "\n",
      "_HF_ENDPOINT = os.getenv(\"HF_ENDPOINT\", \"https://huggingface.co\")\n",
      "\n",
      "_DESCRIPTION = \"BELLE multiturn chat dataset.\"\n",
      "\n",
      "_CITATION = \"\"\"\\\n",
      "@article{belle2023exploring,\n",
      "  title={Exploring the Impact of Instruction Data Scaling on Large Language Models: An Empirical Study on Real-World Use Cases},\n",
      "  author={Yunjie Ji, Yong Deng, Yan Gong, Yiping Peng, Qiang Niu, Lei Zhang, Baochang Ma, Xiangang Li},\n",
      "  journal={arXiv preprint arXiv:2303.14742},\n",
      "  year={2023}\n",
      "}\n",
      "\"\"\"\n",
      "\n",
      "_HOMEPAGE = f\"{_HF_ENDPOINT}/datasets/BelleGroup/multiturn_chat_0.8M\"\n",
      "_LICENSE = \"gpl-3.0\"\n",
      "_URL = f\"{_HF_ENDPOINT}/datasets/BelleGroup/multiturn_chat_0.8M/resolve/main/multiturn_chat_0.8M.json\"\n",
      "\n",
      "\n",
      "class BelleMultiturn(datasets.GeneratorBasedBuilder):\n",
      "    VERSION = datasets.Version(\"0.0.0\")\n",
      "\n",
      "    def _info(self):\n",
      "        features = datasets.Features(\n",
      "            {\"conversations\": [{\"from\": datasets.Value(\"string\"), \"value\": datasets.Value(\"string\")}]}\n",
      "        )\n",
      "        return datasets.DatasetInfo(\n",
      "            description=_DESCRIPTION, features=features, homepage=_HOMEPAGE, license=_LICENSE, citation=_CITATION\n",
      "        )\n",
      "\n",
      "    def _split_generators(self, dl_manager: datasets.DownloadManager):\n",
      "        file_path = dl_manager.download(_URL)\n",
      "        return [datasets.SplitGenerator(name=datasets.Split.TRAIN, gen_kwargs={\"filepath\": file_path})]\n",
      "\n",
      "    def _generate_examples(self, filepath: str):\n",
      "        with open(filepath, encoding=\"utf-8\") as f:\n",
      "            for key, row in enumerate(f):\n",
      "                data = json.loads(row)\n",
      "                conversations = []\n",
      "                prompt = data[\"instruction\"].strip()\n",
      "                response = data[\"output\"].strip()\n",
      "\n",
      "                assist_idx = prompt.rfind(\"Assistant:\")\n",
      "                human_idx = prompt.rfind(\"Human:\")\n",
      "                query = prompt[human_idx + 6 : assist_idx].strip()\n",
      "                prompt = prompt[:human_idx].strip()\n",
      "                conversations.insert(0, {\"from\": \"gpt\", \"value\": response})\n",
      "                conversations.insert(0, {\"from\": \"human\", \"value\": query})\n",
      "\n",
      "                while prompt.rfind(\"Assistant:\") != -1:\n",
      "                    assist_idx = prompt.rfind(\"Assistant:\")\n",
      "                    human_idx = prompt.rfind(\"Human:\")\n",
      "                    if human_idx != -1:\n",
      "                        old_query = prompt[human_idx + 6 : assist_idx].strip()\n",
      "                        old_resp = prompt[assist_idx + 10 :].strip()\n",
      "                        conversations.insert(0, {\"from\": \"gpt\", \"value\": old_resp})\n",
      "                        conversations.insert(0, {\"from\": \"human\", \"value\": old_query})\n",
      "                    else:\n",
      "                        break\n",
      "                    prompt = prompt[:human_idx].strip()\n",
      "\n",
      "                yield key, {\"conversations\": conversations}\n",
      "\n",
      "```\n",
      "\n",
      "\n",
      "FILE NAME: data/hh_rlhf_en/hh_rlhf_en.py\n",
      "\n",
      "\n",
      "import json\n",
      "import os\n",
      "from typing import List\n",
      "\n",
      "import datasets\n",
      "\n",
      "\n",
      "_HF_ENDPOINT = os.getenv(\"HF_ENDPOINT\", \"https://huggingface.co\")\n",
      "_DESCRIPTION = \"Human preference data about helpfulness and harmlessness.\"\n",
      "_CITATION = \"\"\n",
      "_HOMEPAGE = f\"{_HF_ENDPOINT}/datasets/Anthropic/hh-rlhf\"\n",
      "_LICENSE = \"mit\"\n",
      "_URL = f\"{_HF_ENDPOINT}/datasets/Anthropic/hh-rlhf/resolve/main/\"\n",
      "_URLS = {\n",
      "    \"train\": [\n",
      "        _URL + \"harmless-base/train.jsonl.gz\",\n",
      "        _URL + \"helpful-base/train.jsonl.gz\",\n",
      "        _URL + \"helpful-online/train.jsonl.gz\",\n",
      "        _URL + \"helpful-rejection-sampled/train.jsonl.gz\",\n",
      "    ],\n",
      "    \"test\": [\n",
      "        _URL + \"harmless-base/test.jsonl.gz\",\n",
      "        _URL + \"helpful-base/test.jsonl.gz\",\n",
      "        _URL + \"helpful-online/test.jsonl.gz\",\n",
      "        _URL + \"helpful-rejection-sampled/test.jsonl.gz\",\n",
      "    ],\n",
      "}\n",
      "\n",
      "\n",
      "class HhRlhfEn(datasets.GeneratorBasedBuilder):\n",
      "    VERSION = datasets.Version(\"0.0.0\")\n",
      "\n",
      "    def _info(self) -> datasets.DatasetInfo:\n",
      "        features = datasets.Features(\n",
      "            {\n",
      "                \"instruction\": datasets.Value(\"string\"),\n",
      "                \"chosen\": datasets.Value(\"string\"),\n",
      "                \"rejected\": datasets.Value(\"string\"),\n",
      "                \"history\": datasets.Sequence(datasets.Sequence(datasets.Value(\"string\"))),\n",
      "            }\n",
      "        )\n",
      "        return datasets.DatasetInfo(\n",
      "            description=_DESCRIPTION, features=features, homepage=_HOMEPAGE, license=_LICENSE, citation=_CITATION\n",
      "        )\n",
      "\n",
      "    def _split_generators(self, dl_manager: datasets.DownloadManager):\n",
      "        file_path = dl_manager.download_and_extract(_URLS)\n",
      "        return [\n",
      "            datasets.SplitGenerator(name=datasets.Split.TRAIN, gen_kwargs={\"filepaths\": file_path[\"train\"]}),\n",
      "            datasets.SplitGenerator(name=datasets.Split.TEST, gen_kwargs={\"filepaths\": file_path[\"test\"]}),\n",
      "        ]\n",
      "\n",
      "    def _generate_examples(self, filepaths: List[str]):\n",
      "        key = 0\n",
      "        for filepath in filepaths:\n",
      "            with open(filepath, encoding=\"utf-8\") as f:\n",
      "                for row in f:\n",
      "                    data = json.loads(row)\n",
      "                    chosen = data[\"chosen\"]\n",
      "                    rejected = data[\"rejected\"]\n",
      "\n",
      "                    assist_idx = rejected.rfind(\"\\n\\nAssistant: \")\n",
      "                    r_reject = rejected[assist_idx + 13 :].strip()\n",
      "                    assist_idx = chosen.rfind(\"\\n\\nAssistant: \")\n",
      "                    r_accept = chosen[assist_idx + 13 :].strip()\n",
      "\n",
      "                    human_idx = chosen.rfind(\"\\n\\nHuman: \")\n",
      "                    query = chosen[human_idx + 9 : assist_idx].strip()\n",
      "                    prompt = chosen[:human_idx]\n",
      "                    history = []\n",
      "\n",
      "                    while prompt.rfind(\"\\n\\nAssistant: \") != -1:\n",
      "                        assist_idx = prompt.rfind(\"\\n\\nAssistant: \")\n",
      "                        human_idx = prompt.rfind(\"\\n\\nHuman: \")\n",
      "                        if human_idx != -1:\n",
      "                            old_query = prompt[human_idx + 9 : assist_idx].strip()\n",
      "                            old_resp = prompt[assist_idx + 13 :].strip()\n",
      "                            history.insert(0, (old_query, old_resp))\n",
      "                        else:\n",
      "                            break\n",
      "                        prompt = prompt[:human_idx]\n",
      "\n",
      "                    yield key, {\"instruction\": query, \"chosen\": r_accept, \"rejected\": r_reject, \"history\": history}\n",
      "                    key += 1\n",
      "\n",
      "```</s>\n",
      "=========================\n",
      "FILE NAME: data/ultra_chat/ultra_chat.py\n",
      "\n",
      "\n",
      "import json\n",
      "import os\n",
      "from typing import List\n",
      "\n",
      "import datasets\n",
      "\n",
      "\n",
      "_HF_ENDPOINT = os.getenv(\"HF_ENDPOINT\", \"https://huggingface.co\")\n",
      "\n",
      "_DESCRIPTION = \"UltraChat: Large-scale, Informative, and Diverse Multi-round Dialogue Data.\"\n",
      "\n",
      "_CITATION = \"\"\"\\\n",
      "@misc{UltraChat,\n",
      "  author = {Ding, Ning and Chen, Yulin and Xu, Bokai and Hu, Shengding and Qin, Yujia and Liu, Zhiyuan and Sun, Maosong and Zhou, Bowen},\n",
      "  title = {UltraChat: A Large-scale Auto-generated Multi-round Dialogue Data},\n",
      "  year = {2023},\n",
      "  publisher = {GitHub},\n",
      "  journal = {GitHub repository},\n",
      "  howpublished = {\\\\url{https://github.com/thunlp/ultrachat}},\n",
      "}\n",
      "\"\"\"\n",
      "\n",
      "_HOMEPAGE = f\"{_HF_ENDPOINT}/datasets/stingning/ultrachat\"\n",
      "_LICENSE = \"cc-by-nc-4.0\"\n",
      "_BASE_DATA_URL = f\"{_HF_ENDPOINT}/datasets/stingning/ultrachat/resolve/main/train_{{idx}}.jsonl\"\n",
      "\n",
      "\n",
      "class UltraChat(datasets.GeneratorBasedBuilder):\n",
      "    VERSION = datasets.Version(\"0.0.0\")\n",
      "\n",
      "    def _info(self):\n",
      "        features = datasets.Features(\n",
      "            {\"conversations\": [{\"from\": datasets.Value(\"string\"), \"value\": datasets.Value(\"string\")}]}\n",
      "        )\n",
      "        return datasets.DatasetInfo(\n",
      "            description=_DESCRIPTION, features=features, homepage=_HOMEPAGE, license=_LICENSE, citation=_CITATION\n",
      "        )\n",
      "\n",
      "    def _split_generators(self, dl_manager: datasets.DownloadManager):\n",
      "        file_paths = [dl_manager.download(_BASE_DATA_URL.format(idx=idx)) for idx in range(10)]  # multiple shards\n",
      "        return [datasets.SplitGenerator(name=datasets.Split.TRAIN, gen_kwargs={\"filepaths\": file_paths})]\n",
      "\n",
      "    def _generate_examples(self, filepaths: List[str]):\n",
      "        for filepath in filepaths:\n",
      "            with open(filepath, encoding=\"utf-8\") as f:\n",
      "                for row in f:\n",
      "                    try:\n",
      "                        data = json.loads(row)\n",
      "                    except Exception:\n",
      "                        continue\n",
      "                    key: int = data[\"id\"]\n",
      "                    content: List[str] = data[\"data\"]\n",
      "                    if len(content) % 2 == 1:\n",
      "                        content.pop(-1)\n",
      "                    if len(content) < 2:\n",
      "                        continue\n",
      "                    conversations = [\n",
      "                        {\"from\": \"human\" if i % 2 == 0 else \"gpt\", \"value\": content[i]} for i in range(len(content))\n",
      "                    ]\n",
      "                    yield key, {\"conversations\": conversations}\n",
      "\n",
      "```\n",
      "\n",
      "\n",
      "FILE NAME: evaluation/ceval/ceval.py\n",
      "\n",
      "\n",
      "# Copyright 2020 The HuggingFace Datasets Authors and the current dataset script contributor.\n",
      "#\n",
      "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
      "# you may not use this file except in compliance with the License.\n",
      "# You may obtain a copy of the License at\n",
      "#\n",
      "#     http://www.apache.org/licenses/LICENSE-2.0\n",
      "#\n",
      "# Unless required by applicable law or agreed to in writing, software\n",
      "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
      "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
      "# See the License for the specific language governing permissions and\n",
      "# limitations under the License.\n",
      "\n",
      "import os\n",
      "\n",
      "import datasets\n",
      "import pandas as pd\n",
      "\n",
      "\n",
      "_CITATION = \"\"\"\\\n",
      "@article{huang2023ceval,\n",
      "  title={C-Eval: A Multi-Level Multi-Discipline Chinese Evaluation Suite for Foundation Models},\n",
      "  author={Huang, Yuzhen and Bai, Yuzhuo and Zhu, Zhihao and Zhang, Junlei and Zhang, Jinghan and Su, Tangjun and Liu, Junteng and Lv, Chuancheng and Zhang, Yikai and Lei, Jiayi and Fu, Yao and Sun, Maosong and He, Junxian},\n",
      "  journal={arXiv preprint arXiv:2305.08322},\n",
      "  year={2023}\n",
      "}\n",
      "\"\"\"\n",
      "\n",
      "_DESCRIPTION = \"\"\"\\\n",
      "C-Eval is a comprehensive Chinese evaluation suite for foundation models. It consists of 13948 multi-choice questions spanning 52 diverse disciplines and four difficulty levels.\n",
      "\"\"\"\n",
      "\n",
      "_HOMEPAGE = \"https://cevalbenchmark.com\"\n",
      "\n",
      "_LICENSE = \"Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License\"\n",
      "\n",
      "_URL = \"ceval.zip\"\n",
      "\n",
      "task_list = [\n",
      "    \"computer_network\",\n",
      "    \"operating_system\",\n",
      "    \"computer_architecture\",\n",
      "    \"college_programming\",\n",
      "    \"college_physics\",\n",
      "    \"college_chemistry\",\n",
      "    \"advanced_mathematics\",\n",
      "    \"probability_and_statistics\",\n",
      "    \"discrete_mathematics\",\n",
      "    \"electrical_engineer\",\n",
      "    \"metrology_engineer\",\n",
      "    \"high_school_mathematics\",\n",
      "    \"high_school_physics\",\n",
      "    \"high_school_chemistry\",\n",
      "    \"high_school_biology\",\n",
      "    \"middle_school_mathematics\",\n",
      "    \"middle_school_biology\",\n",
      "    \"middle_school_physics\",\n",
      "    \"middle_school_chemistry\",\n",
      "    \"veterinary_medicine\",\n",
      "    \"college_economics\",\n",
      "    \"business_administration\",\n",
      "    \"marxism\",\n",
      "    \"mao_zedong_thought\",\n",
      "    \"education_science\",\n",
      "    \"teacher_qualification\",\n",
      "    \"high_school_politics\",\n",
      "    \"high_school_geography\",\n",
      "    \"middle_school_politics\",\n",
      "    \"middle_school_geography\",\n",
      "    \"modern_chinese_history\",\n",
      "    \"ideological_and_moral_cultivation\",\n",
      "    \"logic\",\n",
      "    \"law\",\n",
      "    \"chinese_language_and_literature\",\n",
      "    \"art_studies\",\n",
      "    \"professional_tour_guide\",\n",
      "    \"legal_professional\",\n",
      "    \"high_school_chinese\",\n",
      "    \"high_school_history\",\n",
      "    \"middle_school_history\",\n",
      "    \"civil_servant\",\n",
      "    \"sports_science\",\n",
      "    \"plant_protection\",\n",
      "    \"basic_medicine\",\n",
      "    \"clinical_medicine\",\n",
      "    \"urban_and_rural_planner\",\n",
      "    \"accountant\",\n",
      "    \"fire_engineer\",\n",
      "    \"environmental_impact_assessment_engineer\",\n",
      "    \"tax_accountant\",\n",
      "    \"physician\",\n",
      "]\n",
      "\n",
      "\n",
      "class CevalConfig(datasets.BuilderConfig):\n",
      "    def __init__(self, **kwargs):\n",
      "        super().__init__(version=datasets.Version(\"1.0.0\"), **kwargs)\n",
      "\n",
      "\n",
      "class Ceval(datasets.GeneratorBasedBuilder):\n",
      "    BUILDER_CONFIGS = [\n",
      "        CevalConfig(\n",
      "            name=task_name,\n",
      "        )\n",
      "        for task_name in task_list\n",
      "    ]\n",
      "\n",
      "    def _info(self):\n",
      "        features = datasets.Features(\n",
      "            {\n",
      "                \"id\": datasets.Value(\"int32\"),\n",
      "                \"question\": datasets.Value(\"string\"),\n",
      "                \"A\": datasets.Value(\"string\"),\n",
      "                \"B\": datasets.Value(\"string\"),\n",
      "                \"C\": datasets.Value(\"string\"),\n",
      "                \"D\": datasets.Value(\"string\"),\n",
      "                \"answer\": datasets.Value(\"string\"),\n",
      "                \"explanation\": datasets.Value(\"string\"),\n",
      "            }\n",
      "        )\n",
      "        return datasets.DatasetInfo(\n",
      "            description=_DESCRIPTION,\n",
      "            features=features,\n",
      "            homepage=_HOMEPAGE,\n",
      "            license=_LICENSE,\n",
      "            citation=_CITATION,\n",
      "        )\n",
      "\n",
      "    def _split_generators(self, dl_manager):\n",
      "        data_dir = dl_manager.download_and_extract(_URL)\n",
      "        task_name = self.config.name\n",
      "        return [\n",
      "            datasets.SplitGenerator(\n",
      "                name=datasets.Split.TEST,\n",
      "                gen_kwargs={\n",
      "                    \"filepath\": os.path.join(data_dir, \"test\", f\"{task_name}_test.csv\"),\n",
      "                },\n",
      "            ),\n",
      "            datasets.SplitGenerator(\n",
      "                name=datasets.Split.VALIDATION,\n",
      "                gen_kwargs={\n",
      "                    \"filepath\": os.path.join(data_dir, \"val\", f\"{task_name}_val.csv\"),\n",
      "                },\n",
      "            ),\n",
      "            datasets.SplitGenerator(\n",
      "                name=datasets.Split.TRAIN,\n",
      "                gen_kwargs={\n",
      "                    \"filepath\": os.path.join(data_dir, \"dev\", f\"{task_name}_dev.csv\"),\n",
      "                },\n",
      "            ),\n",
      "        ]\n",
      "\n",
      "    def _generate_examples(self, filepath):\n",
      "        df = pd.read_csv(filepath, encoding=\"utf-8\")\n",
      "        for i, instance in enumerate(df.to_dict(orient=\"records\")):\n",
      "            if \"answer\" not in instance.keys():\n",
      "                instance[\"answer\"] = \"\"\n",
      "            if \"explanation\" not in instance.keys():\n",
      "                instance[\"explanation\"] = \"\"\n",
      "            yield i, instance\n",
      "\n",
      "```</s>\n",
      "=========================\n",
      "FILE NAME: evaluation/cmmlu/cmmlu.py\n",
      "\n",
      "\n",
      "# Copyright 2020 The HuggingFace Datasets Authors and the current dataset script contributor.\n",
      "#\n",
      "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
      "# you may not use this file except in compliance with the License.\n",
      "# You may obtain a copy of the License at\n",
      "#\n",
      "#     http://www.apache.org/licenses/LICENSE-2.0\n",
      "#\n",
      "# Unless required by applicable law or agreed to in writing, software\n",
      "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
      "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
      "# See the License for the specific language governing permissions and\n",
      "# limitations under the License.\n",
      "\n",
      "import os\n",
      "\n",
      "import datasets\n",
      "import pandas as pd\n",
      "\n",
      "\n",
      "_CITATION = \"\"\"\\\n",
      "@article{li2023cmmlu,\n",
      "  title={CMMLU: Measuring massive multitask language understanding in Chinese},\n",
      "  author={Haonan Li and Yixuan Zhang and Fajri Koto and Yifei Yang and Hai Zhao and Yeyun Gong and Nan Duan and Timothy Baldwin},\n",
      "  journal={arXiv preprint arXiv:2306.09212},\n",
      "  year={2023}\n",
      "}\n",
      "\"\"\"\n",
      "\n",
      "_DESCRIPTION = \"\"\"\\\n",
      "CMMLU is a comprehensive Chinese assessment suite specifically designed to evaluate the advanced knowledge and reasoning abilities of LLMs within the Chinese language and cultural context.\n",
      "\"\"\"\n",
      "\n",
      "_HOMEPAGE = \"https://github.com/haonan-li/CMMLU\"\n",
      "\n",
      "_LICENSE = \"Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License\"\n",
      "\n",
      "_URL = \"cmmlu.zip\"\n",
      "\n",
      "task_list = [\n",
      "    \"agronomy\",\n",
      "    \"anatomy\",\n",
      "    \"ancient_chinese\",\n",
      "    \"arts\",\n",
      "    \"astronomy\",\n",
      "    \"business_ethics\",\n",
      "    \"chinese_civil_service_exam\",\n",
      "    \"chinese_driving_rule\",\n",
      "    \"chinese_food_culture\",\n",
      "    \"chinese_foreign_policy\",\n",
      "    \"chinese_history\",\n",
      "    \"chinese_literature\",\n",
      "    \"chinese_teacher_qualification\",\n",
      "    \"clinical_knowledge\",\n",
      "    \"college_actuarial_science\",\n",
      "    \"college_education\",\n",
      "    \"college_engineering_hydrology\",\n",
      "    \"college_law\",\n",
      "    \"college_mathematics\",\n",
      "    \"college_medical_statistics\",\n",
      "    \"college_medicine\",\n",
      "    \"computer_science\",\n",
      "    \"computer_security\",\n",
      "    \"conceptual_physics\",\n",
      "    \"construction_project_management\",\n",
      "    \"economics\",\n",
      "    \"education\",\n",
      "    \"electrical_engineering\",\n",
      "    \"elementary_chinese\",\n",
      "    \"elementary_commonsense\",\n",
      "    \"elementary_information_and_technology\",\n",
      "    \"elementary_mathematics\",\n",
      "    \"ethnology\",\n",
      "    \"food_science\",\n",
      "    \"genetics\",\n",
      "    \"global_facts\",\n",
      "    \"high_school_biology\",\n",
      "    \"high_school_chemistry\",\n",
      "    \"high_school_geography\",\n",
      "    \"high_school_mathematics\",\n",
      "    \"high_school_physics\",\n",
      "    \"high_school_politics\",\n",
      "    \"human_sexuality\",\n",
      "    \"international_law\",\n",
      "    \"journalism\",\n",
      "    \"jurisprudence\",\n",
      "    \"legal_and_moral_basis\",\n",
      "    \"logical\",\n",
      "    \"machine_learning\",\n",
      "    \"management\",\n",
      "    \"marketing\",\n",
      "    \"marxist_theory\",\n",
      "    \"modern_chinese\",\n",
      "    \"nutrition\",\n",
      "    \"philosophy\",\n",
      "    \"professional_accounting\",\n",
      "    \"professional_law\",\n",
      "    \"professional_medicine\",\n",
      "    \"professional_psychology\",\n",
      "    \"public_relations\",\n",
      "    \"security_study\",\n",
      "    \"sociology\",\n",
      "    \"sports_science\",\n",
      "    \"traditional_chinese_medicine\",\n",
      "    \"virology\",\n",
      "    \"world_history\",\n",
      "    \"world_religions\",\n",
      "]\n",
      "\n",
      "\n",
      "class CMMLUConfig(datasets.BuilderConfig):\n",
      "    def __init__(self, **kwargs):\n",
      "        super().__init__(version=datasets.Version(\"1.0.1\"), **kwargs)\n",
      "\n",
      "\n",
      "class CMMLU(datasets.GeneratorBasedBuilder):\n",
      "    BUILDER_CONFIGS = [\n",
      "        CMMLUConfig(\n",
      "            name=task_name,\n",
      "        )\n",
      "        for task_name in task_list\n",
      "    ]\n",
      "\n",
      "    def _info(self):\n",
      "        features = datasets.Features(\n",
      "            {\n",
      "                \"question\": datasets.Value(\"string\"),\n",
      "                \"A\": datasets.Value(\"string\"),\n",
      "                \"B\": datasets.Value(\"string\"),\n",
      "                \"C\": datasets.Value(\"string\"),\n",
      "                \"D\": datasets.Value(\"string\"),\n",
      "                \"answer\": datasets.Value(\"string\"),\n",
      "            }\n",
      "        )\n",
      "        return datasets.DatasetInfo(\n",
      "            description=_DESCRIPTION,\n",
      "            features=features,\n",
      "            homepage=_HOMEPAGE,\n",
      "            license=_LICENSE,\n",
      "            citation=_CITATION,\n",
      "        )\n",
      "\n",
      "    def _split_generators(self, dl_manager):\n",
      "        data_dir = dl_manager.download_and_extract(_URL)\n",
      "        task_name = self.config.name\n",
      "        return [\n",
      "            datasets.SplitGenerator(\n",
      "                name=datasets.Split.TEST,\n",
      "                gen_kwargs={\n",
      "                    \"filepath\": os.path.join(data_dir, f\"test/{task_name}.csv\"),\n",
      "                },\n",
      "            ),\n",
      "            datasets.SplitGenerator(\n",
      "                name=datasets.Split.TRAIN,\n",
      "                gen_kwargs={\n",
      "                    \"filepath\": os.path.join(data_dir, f\"dev/{task_name}.csv\"),\n",
      "                },\n",
      "            ),\n",
      "        ]\n",
      "\n",
      "    def _generate_examples(self, filepath):\n",
      "        df = pd.read_csv(filepath, header=0, index_col=0, encoding=\"utf-8\")\n",
      "        for i, instance in enumerate(df.to_dict(orient=\"records\")):\n",
      "            question = instance.pop(\"Question\", \"\")\n",
      "            answer = instance.pop(\"Answer\", \"\")\n",
      "            instance[\"question\"] = question\n",
      "            instance[\"answer\"] = answer\n",
      "            yield i, instance\n",
      "\n",
      "```\n",
      "\n",
      "\n",
      "FILE NAME: evaluation/mmlu/mmlu.py\n",
      "\n",
      "\n",
      "# Copyright 2020 The HuggingFace Datasets Authors and the current dataset script contributor.\n",
      "#\n",
      "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
      "# you may not use this file except in compliance with the License.\n",
      "# You may obtain a copy of the License at\n",
      "#\n",
      "#     http://www.apache.org/licenses/LICENSE-2.0\n",
      "#\n",
      "# Unless required by applicable law or agreed to in writing, software\n",
      "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
      "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
      "# See the License for the specific language governing permissions and\n",
      "# limitations under the License.\n",
      "\n",
      "import os\n",
      "\n",
      "import datasets\n",
      "import pandas as pd\n",
      "\n",
      "\n",
      "_CITATION = \"\"\"\\\n",
      "@article{hendryckstest2021,\n",
      "  title={Measuring Massive Multitask Language Understanding},\n",
      "  author={Dan Hendrycks and Collin Burns and Steven Basart and Andy Zou and Mantas Mazeika and Dawn Song and Jacob Steinhardt},\n",
      "  journal={Proceedings of the International Conference on Learning Representations (ICLR)},\n",
      "  year={2021}\n",
      "}\n",
      "\"\"\"\n",
      "\n",
      "_DESCRIPTION = \"\"\"\\\n",
      "Measuring Massive Multitask Language Understanding by Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt (ICLR 2021).\n",
      "\"\"\"\n",
      "\n",
      "_HOMEPAGE = \"https://github.com/hendrycks/test\"\n",
      "\n",
      "_LICENSE = \"MIT\"\n",
      "\n",
      "_URL = \"mmlu.zip\"\n",
      "\n",
      "task_list = [\n",
      "    \"high_school_european_history\",\n",
      "    \"business_ethics\",\n",
      "    \"clinical_knowledge\",\n",
      "    \"medical_genetics\",\n",
      "    \"high_school_us_history\",\n",
      "    \"high_school_physics\",\n",
      "    \"high_school_world_history\",\n",
      "    \"virology\",\n",
      "    \"high_school_microeconomics\",\n",
      "    \"econometrics\",\n",
      "    \"college_computer_science\",\n",
      "    \"high_school_biology\",\n",
      "    \"abstract_algebra\",\n",
      "    \"professional_accounting\",\n",
      "    \"philosophy\",\n",
      "    \"professional_medicine\",\n",
      "    \"nutrition\",\n",
      "    \"global_facts\",\n",
      "    \"machine_learning\",\n",
      "    \"security_studies\",\n",
      "    \"public_relations\",\n",
      "    \"professional_psychology\",\n",
      "    \"prehistory\",\n",
      "    \"anatomy\",\n",
      "    \"human_sexuality\",\n",
      "    \"college_medicine\",\n",
      "    \"high_school_government_and_politics\",\n",
      "    \"college_chemistry\",\n",
      "    \"logical_fallacies\",\n",
      "    \"high_school_geography\",\n",
      "    \"elementary_mathematics\",\n",
      "    \"human_aging\",\n",
      "    \"college_mathematics\",\n",
      "    \"high_school_psychology\",\n",
      "    \"formal_logic\",\n",
      "    \"high_school_statistics\",\n",
      "    \"international_law\",\n",
      "    \"high_school_mathematics\",\n",
      "    \"high_school_computer_science\",\n",
      "    \"conceptual_physics\",\n",
      "    \"miscellaneous\",\n",
      "    \"high_school_chemistry\",\n",
      "    \"marketing\",\n",
      "    \"professional_law\",\n",
      "    \"management\",\n",
      "    \"college_physics\",\n",
      "    \"jurisprudence\",\n",
      "    \"world_religions\",\n",
      "    \"sociology\",\n",
      "    \"us_foreign_policy\",\n",
      "    \"high_school_macroeconomics\",\n",
      "    \"computer_security\",\n",
      "    \"moral_scenarios\",\n",
      "    \"moral_disputes\",\n",
      "    \"electrical_engineering\",\n",
      "    \"astronomy\",\n",
      "    \"college_biology\",\n",
      "]\n",
      "\n",
      "\n",
      "class MMLUConfig(datasets.BuilderConfig):\n",
      "    def __init__(self, **kwargs):\n",
      "        super().__init__(version=datasets.Version(\"1.0.0\"), **kwargs)\n",
      "\n",
      "\n",
      "class MMLU(datasets.GeneratorBasedBuilder):\n",
      "    BUILDER_CONFIGS = [\n",
      "        MMLUConfig(\n",
      "            name=task_name,\n",
      "        )\n",
      "        for task_name in task_list\n",
      "    ]\n",
      "\n",
      "    def _info(self):\n",
      "        features = datasets.Features(\n",
      "            {\n",
      "                \"question\": datasets.Value(\"string\"),\n",
      "                \"A\": datasets.Value(\"string\"),\n",
      "                \"B\": datasets.Value(\"string\"),\n",
      "                \"C\": datasets.Value(\"string\"),\n",
      "                \"D\": datasets.Value(\"string\"),\n",
      "                \"answer\": datasets.Value(\"string\"),\n",
      "            }\n",
      "        )\n",
      "        return datasets.DatasetInfo(\n",
      "            description=_DESCRIPTION,\n",
      "            features=features,\n",
      "            homepage=_HOMEPAGE,\n",
      "            license=_LICENSE,\n",
      "            citation=_CITATION,\n",
      "        )\n",
      "\n",
      "    def _split_generators(self, dl_manager):\n",
      "        data_dir = dl_manager.download_and_extract(_URL)\n",
      "        task_name = self.config.name\n",
      "        return [\n",
      "            datasets.SplitGenerator(\n",
      "                name=datasets.Split.TEST,\n",
      "                gen_kwargs={\n",
      "                    \"filepath\": os.path.join(data_dir, \"data\", \"test\", f\"{task_name}_test.csv\"),\n",
      "                },\n",
      "            ),\n",
      "            datasets.SplitGenerator(\n",
      "                name=datasets.Split.VALIDATION,\n",
      "                gen_kwargs={\n",
      "                    \"filepath\": os.path.join(data_dir, \"data\", \"val\", f\"{task_name}_val.csv\"),\n",
      "                },\n",
      "            ),\n",
      "            datasets.SplitGenerator(\n",
      "                name=datasets.Split.TRAIN,\n",
      "                gen_kwargs={\n",
      "                    \"filepath\": os.path.join(data_dir, \"data\", \"dev\", f\"{task_name}_dev.csv\"),\n",
      "                },\n",
      "            ),\n",
      "        ]\n",
      "\n",
      "    def _generate_examples(self, filepath):\n",
      "        df = pd.read_csv(filepath, header=None)\n",
      "        df.columns = [\"question\", \"A\", \"B\", \"C\", \"D\", \"answer\"]\n",
      "\n",
      "        yield from enumerate(df.to_dict(orient=\"records\"))\n",
      "\n",
      "```</s>\n",
      "=========================\n",
      "FILE NAME: scripts/api_example/test_image.py\n",
      "\n",
      "\n",
      "# Copyright 2025 the LlamaFactory team.\n",
      "#\n",
      "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
      "# you may not use this file except in compliance with the License.\n",
      "# You may obtain a copy of the License at\n",
      "#\n",
      "#     http://www.apache.org/licenses/LICENSE-2.0\n",
      "#\n",
      "# Unless required by applicable law or agreed to in writing, software\n",
      "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
      "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
      "# See the License for the specific language governing permissions and\n",
      "# limitations under the License.\n",
      "\n",
      "import os\n",
      "\n",
      "from openai import OpenAI\n",
      "from transformers.utils.versions import require_version\n",
      "\n",
      "\n",
      "require_version(\"openai>=1.5.0\", \"To fix: pip install openai>=1.5.0\")\n",
      "\n",
      "\n",
      "def main():\n",
      "    client = OpenAI(\n",
      "        api_key=\"{}\".format(os.environ.get(\"API_KEY\", \"0\")),\n",
      "        base_url=\"http://localhost:{}/v1\".format(os.environ.get(\"API_PORT\", 8000)),\n",
      "    )\n",
      "    messages = []\n",
      "    messages.append(\n",
      "        {\n",
      "            \"role\": \"user\",\n",
      "            \"content\": [\n",
      "                {\"type\": \"text\", \"text\": \"Output the color and number of each box.\"},\n",
      "                {\n",
      "                    \"type\": \"image_url\",\n",
      "                    \"image_url\": {\"url\": \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-VL/boxes.png\"},\n",
      "                },\n",
      "            ],\n",
      "        }\n",
      "    )\n",
      "    result = client.chat.completions.create(messages=messages, model=\"test\")\n",
      "    messages.append(result.choices[0].message)\n",
      "    print(\"Round 1:\", result.choices[0].message.content)\n",
      "    # The image shows a pyramid of colored blocks with numbers on them. Here are the colors and numbers of ...\n",
      "    messages.append(\n",
      "        {\n",
      "            \"role\": \"user\",\n",
      "            \"content\": [\n",
      "                {\"type\": \"text\", \"text\": \"What kind of flower is this?\"},\n",
      "                {\n",
      "                    \"type\": \"image_url\",\n",
      "                    \"image_url\": {\"url\": \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-VL/flowers.jpg\"},\n",
      "                },\n",
      "            ],\n",
      "        }\n",
      "    )\n",
      "    result = client.chat.completions.create(messages=messages, model=\"test\")\n",
      "    messages.append(result.choices[0].message)\n",
      "    print(\"Round 2:\", result.choices[0].message.content)\n",
      "    # The image shows a cluster of forget-me-not flowers. Forget-me-nots are small ...\n",
      "\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    main()\n",
      "\n",
      "```\n",
      "\n",
      "\n",
      "FILE NAME: scripts/api_example/test_toolcall.py\n",
      "\n",
      "\n",
      "# Copyright 2025 the LlamaFactory team.\n",
      "#\n",
      "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
      "# you may not use this file except in compliance with the License.\n",
      "# You may obtain a copy of the License at\n",
      "#\n",
      "#     http://www.apache.org/licenses/LICENSE-2.0\n",
      "#\n",
      "# Unless required by applicable law or agreed to in writing, software\n",
      "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
      "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
      "# See the License for the specific language governing permissions and\n",
      "# limitations under the License.\n",
      "\n",
      "import json\n",
      "import os\n",
      "from typing import Sequence\n",
      "\n",
      "from openai import OpenAI\n",
      "from transformers.utils.versions import require_version\n",
      "\n",
      "\n",
      "require_version(\"openai>=1.5.0\", \"To fix: pip install openai>=1.5.0\")\n",
      "\n",
      "\n",
      "def calculate_gpa(grades: Sequence[str], hours: Sequence[int]) -> float:\n",
      "    grade_to_score = {\"A\": 4, \"B\": 3, \"C\": 2}\n",
      "    total_score, total_hour = 0, 0\n",
      "    for grade, hour in zip(grades, hours):\n",
      "        total_score += grade_to_score[grade] * hour\n",
      "        total_hour += hour\n",
      "    return round(total_score / total_hour, 2)\n",
      "\n",
      "\n",
      "def main():\n",
      "    client = OpenAI(\n",
      "        api_key=\"{}\".format(os.environ.get(\"API_KEY\", \"0\")),\n",
      "        base_url=\"http://localhost:{}/v1\".format(os.environ.get(\"API_PORT\", 8000)),\n",
      "    )\n",
      "    tools = [\n",
      "        {\n",
      "            \"type\": \"function\",\n",
      "            \"function\": {\n",
      "                \"name\": \"calculate_gpa\",\n",
      "                \"description\": \"Calculate the Grade Point Average (GPA) based on grades and credit hours\",\n",
      "                \"parameters\": {\n",
      "                    \"type\": \"object\",\n",
      "                    \"properties\": {\n",
      "                        \"grades\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}, \"description\": \"The grades\"},\n",
      "                        \"hours\": {\"type\": \"array\", \"items\": {\"type\": \"integer\"}, \"description\": \"The credit hours\"},\n",
      "                    },\n",
      "                    \"required\": [\"grades\", \"hours\"],\n",
      "                },\n",
      "            },\n",
      "        }\n",
      "    ]\n",
      "    tool_map = {\"calculate_gpa\": calculate_gpa}\n",
      "\n",
      "    messages = []\n",
      "    messages.append({\"role\": \"user\", \"content\": \"My grades are A, A, B, and C. The credit hours are 3, 4, 3, and 2.\"})\n",
      "    result = client.chat.completions.create(messages=messages, model=\"test\", tools=tools)\n",
      "    if result.choices[0].message.tool_calls is None:\n",
      "        raise ValueError(\"Cannot retrieve function call from the response.\")\n",
      "\n",
      "    messages.append(result.choices[0].message)\n",
      "    tool_call = result.choices[0].message.tool_calls[0].function\n",
      "    print(tool_call)\n",
      "    # Function(arguments='{\"grades\": [\"A\", \"A\", \"B\", \"C\"], \"hours\": [3, 4, 3, 2]}', name='calculate_gpa')\n",
      "    name, arguments = tool_call.name, json.loads(tool_call.arguments)\n",
      "    tool_result = tool_map[name](**arguments)\n",
      "    messages.append({\"role\": \"tool\", \"content\": json.dumps({\"gpa\": tool_result}, ensure_ascii=False)})\n",
      "    result = client.chat.completions.create(messages=messages, model=\"test\", tools=tools)\n",
      "    print(result.choices[0].message.content)\n",
      "    # Based on the grades and credit hours you provided, your Grade Point Average (GPA) is 3.42.\n",
      "\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    main()\n",
      "\n",
      "```</s>\n",
      "=========================\n",
      "FILE NAME: scripts/convert_ckpt/llamafy_baichuan2.py\n",
      "\n",
      "\n",
      "# Copyright 2025 the LlamaFactory team.\n",
      "#\n",
      "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
      "# you may not use this file except in compliance with the License.\n",
      "# You may obtain a copy of the License at\n",
      "#\n",
      "#     http://www.apache.org/licenses/LICENSE-2.0\n",
      "#\n",
      "# Unless required by applicable law or agreed to in writing, software\n",
      "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
      "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
      "# See the License for the specific language governing permissions and\n",
      "# limitations under the License.\n",
      "\n",
      "import json\n",
      "import os\n",
      "from collections import OrderedDict\n",
      "from typing import Any, Dict\n",
      "\n",
      "import fire\n",
      "import torch\n",
      "from huggingface_hub import split_torch_state_dict_into_shards\n",
      "from safetensors.torch import save_file\n",
      "from tqdm import tqdm\n",
      "from transformers.modeling_utils import SAFE_WEIGHTS_INDEX_NAME, SAFE_WEIGHTS_NAME, WEIGHTS_INDEX_NAME, WEIGHTS_NAME\n",
      "\n",
      "\n",
      "CONFIG_NAME = \"config.json\"\n",
      "\n",
      "\n",
      "def save_weight(input_dir: str, output_dir: str, shard_size: str, save_safetensors: bool):\n",
      "    baichuan2_state_dict: Dict[str, torch.Tensor] = OrderedDict()\n",
      "    for filepath in tqdm(os.listdir(input_dir), desc=\"Load weights\"):\n",
      "        if os.path.isfile(os.path.join(input_dir, filepath)) and filepath.endswith(\".bin\"):\n",
      "            shard_weight = torch.load(os.path.join(input_dir, filepath), map_location=\"cpu\")\n",
      "            baichuan2_state_dict.update(shard_weight)\n",
      "\n",
      "    llama_state_dict: Dict[str, torch.Tensor] = OrderedDict()\n",
      "    for key, value in tqdm(baichuan2_state_dict.items(), desc=\"Convert format\"):\n",
      "        if \"W_pack\" in key:\n",
      "            proj_size = value.size(0) // 3\n",
      "            llama_state_dict[key.replace(\"W_pack\", \"q_proj\")] = value[:proj_size, :]\n",
      "            llama_state_dict[key.replace(\"W_pack\", \"k_proj\")] = value[proj_size : 2 * proj_size, :]\n",
      "            llama_state_dict[key.replace(\"W_pack\", \"v_proj\")] = value[2 * proj_size :, :]\n",
      "        elif \"lm_head\" in key:\n",
      "            llama_state_dict[key] = torch.nn.functional.normalize(value)\n",
      "        else:\n",
      "            llama_state_dict[key] = value\n",
      "\n",
      "    weights_name = SAFE_WEIGHTS_NAME if save_safetensors else WEIGHTS_NAME\n",
      "    filename_pattern = weights_name.replace(\".bin\", \"{suffix}.bin\").replace(\".safetensors\", \"{suffix}.safetensors\")\n",
      "    state_dict_split = split_torch_state_dict_into_shards(\n",
      "        llama_state_dict, filename_pattern=filename_pattern, max_shard_size=shard_size\n",
      "    )\n",
      "    for shard_file, tensors in tqdm(state_dict_split.filename_to_tensors.items(), desc=\"Save weights\"):\n",
      "        shard = {tensor: llama_state_dict[tensor].contiguous() for tensor in tensors}\n",
      "        if save_safetensors:\n",
      "            save_file(shard, os.path.join(output_dir, shard_file), metadata={\"format\": \"pt\"})\n",
      "        else:\n",
      "            torch.save(shard, os.path.join(output_dir, shard_file))\n",
      "\n",
      "    if not state_dict_split.is_sharded:\n",
      "        print(f\"Model weights saved in {os.path.join(output_dir, weights_name)}.\")\n",
      "    else:\n",
      "        index = {\n",
      "            \"metadata\": state_dict_split.metadata,\n",
      "            \"weight_map\": state_dict_split.tensor_to_filename,\n",
      "        }\n",
      "        index_name = SAFE_WEIGHTS_INDEX_NAME if save_safetensors else WEIGHTS_INDEX_NAME\n",
      "        with open(os.path.join(output_dir, index_name), \"w\", encoding=\"utf-8\") as f:\n",
      "            json.dump(index, f, indent=2, sort_keys=True)\n",
      "\n",
      "        print(f\"Model weights saved in {output_dir}.\")\n",
      "\n",
      "\n",
      "def save_config(input_dir: str, output_dir: str):\n",
      "    with open(os.path.join(input_dir, CONFIG_NAME), encoding=\"utf-8\") as f:\n",
      "        llama2_config_dict: Dict[str, Any] = json.load(f)\n",
      "\n",
      "    llama2_config_dict[\"architectures\"] = [\"LlamaForCausalLM\"]\n",
      "    llama2_config_dict.pop(\"auto_map\", None)\n",
      "    llama2_config_dict.pop(\"tokenizer_class\", None)\n",
      "    llama2_config_dict[\"model_type\"] = \"llama\"\n",
      "\n",
      "    with open(os.path.join(output_dir, CONFIG_NAME), \"w\", encoding=\"utf-8\") as f:\n",
      "        json.dump(llama2_config_dict, f, indent=2)\n",
      "\n",
      "    print(f\"Model config saved in {os.path.join(output_dir, CONFIG_NAME)}\")\n",
      "\n",
      "\n",
      "def llamafy_baichuan2(\n",
      "    input_dir: str,\n",
      "    output_dir: str,\n",
      "    shard_size: str = \"2GB\",\n",
      "    save_safetensors: bool = True,\n",
      "):\n",
      "    r\"\"\"\n",
      "    Converts the Baichuan2-7B model in the same format as LLaMA2-7B.\n",
      "    Usage: python llamafy_baichuan2.py --input_dir input --output_dir output\n",
      "    Converted model: https://huggingface.co/hiyouga/Baichuan2-7B-Base-LLaMAfied\n",
      "    \"\"\"\n",
      "    try:\n",
      "        os.makedirs(output_dir, exist_ok=False)\n",
      "    except Exception as e:\n",
      "        raise print(\"Output dir already exists\", e)\n",
      "\n",
      "    save_weight(input_dir, output_dir, shard_size, save_safetensors)\n",
      "    save_config(input_dir, output_dir)\n",
      "\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    fire.Fire(llamafy_baichuan2)\n",
      "\n",
      "```\n",
      "\n",
      "\n",
      "FILE NAME: scripts/convert_ckpt/llamafy_qwen.py\n",
      "\n",
      "\n",
      "# Copyright 2025 the LlamaFactory team.\n",
      "#\n",
      "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
      "# you may not use this file except in compliance with the License.\n",
      "# You may obtain a copy of the License at\n",
      "#\n",
      "#     http://www.apache.org/licenses/LICENSE-2.0\n",
      "#\n",
      "# Unless required by applicable law or agreed to in writing, software\n",
      "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
      "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
      "# See the License for the specific language governing permissions and\n",
      "# limitations under the License.\n",
      "\n",
      "import json\n",
      "import os\n",
      "from collections import OrderedDict\n",
      "from typing import Any, Dict\n",
      "\n",
      "import fire\n",
      "import torch\n",
      "from huggingface_hub import split_torch_state_dict_into_shards\n",
      "from safetensors import safe_open\n",
      "from safetensors.torch import save_file\n",
      "from tqdm import tqdm\n",
      "from transformers.modeling_utils import SAFE_WEIGHTS_INDEX_NAME, SAFE_WEIGHTS_NAME, WEIGHTS_INDEX_NAME, WEIGHTS_NAME\n",
      "from transformers.utils import check_min_version\n",
      "\n",
      "\n",
      "try:\n",
      "    check_min_version(\"4.34.0\")\n",
      "except Exception:\n",
      "    raise ValueError(\"Please upgrade `transformers` to 4.34.0\")\n",
      "\n",
      "\n",
      "CONFIG_NAME = \"config.json\"\n",
      "\n",
      "\n",
      "def save_weight(input_dir: str, output_dir: str, shard_size: str, save_safetensors: bool) -> str:\n",
      "    qwen_state_dict: Dict[str, torch.Tensor] = OrderedDict()\n",
      "    for filepath in tqdm(os.listdir(input_dir), desc=\"Load weights\"):\n",
      "        if os.path.isfile(os.path.join(input_dir, filepath)) and filepath.endswith(\".safetensors\"):\n",
      "            with safe_open(os.path.join(input_dir, filepath), framework=\"pt\", device=\"cpu\") as f:\n",
      "                for key in f.keys():\n",
      "                    qwen_state_dict[key] = f.get_tensor(key)\n",
      "\n",
      "    llama_state_dict: Dict[str, torch.Tensor] = OrderedDict()\n",
      "    torch_dtype = None\n",
      "    for key, value in tqdm(qwen_state_dict.items(), desc=\"Convert format\"):\n",
      "        if torch_dtype is None:\n",
      "            torch_dtype = value.dtype\n",
      "        if \"wte\" in key:\n",
      "            llama_state_dict[\"model.embed_tokens.weight\"] = value\n",
      "        elif \"ln_f\" in key:\n",
      "            llama_state_dict[\"model.norm.weight\"] = value\n",
      "        else:\n",
      "            key = key.replace(\"transformer.h\", \"model.layers\")\n",
      "            if \"attn.c_attn\" in key:\n",
      "                proj_size = value.size(0) // 3\n",
      "                llama_state_dict[key.replace(\"attn.c_attn\", \"self_attn.q_proj\")] = value[:proj_size, ...]\n",
      "                llama_state_dict[key.replace(\"attn.c_attn\", \"self_attn.k_proj\")] = value[\n",
      "                    proj_size : 2 * proj_size, ...\n",
      "                ]\n",
      "                llama_state_dict[key.replace(\"attn.c_attn\", \"self_attn.v_proj\")] = value[2 * proj_size :, ...]\n",
      "            elif \"attn.c_proj\" in key:\n",
      "                llama_state_dict[key.replace(\"attn.c_proj\", \"self_attn.o_proj\")] = value\n",
      "                llama_state_dict[key.replace(\"attn.c_proj.weight\", \"self_attn.o_proj.bias\")] = torch.zeros_like(\n",
      "                    value[:, 0]\n",
      "                ).squeeze()\n",
      "            elif \"ln_1\" in key:\n",
      "                llama_state_dict[key.replace(\"ln_1\", \"input_layernorm\")] = value\n",
      "            elif \"ln_2\" in key:\n",
      "                llama_state_dict[key.replace(\"ln_2\", \"post_attention_layernorm\")] = value\n",
      "            elif \"mlp.w1\" in key:\n",
      "                llama_state_dict[key.replace(\"mlp.w1\", \"mlp.up_proj\")] = value\n",
      "            elif \"mlp.w2\" in key:\n",
      "                llama_state_dict[key.replace(\"mlp.w2\", \"mlp.gate_proj\")] = value\n",
      "            elif \"mlp.c_proj\" in key:\n",
      "                llama_state_dict[key.replace(\"mlp.c_proj\", \"mlp.down_proj\")] = value\n",
      "            elif \"lm_head\" in key:\n",
      "                llama_state_dict[key] = value\n",
      "            else:\n",
      "                raise KeyError(f\"Unable to process key {key}\")\n",
      "\n",
      "    weights_name = SAFE_WEIGHTS_NAME if save_safetensors else WEIGHTS_NAME\n",
      "    filename_pattern = weights_name.replace(\".bin\", \"{suffix}.bin\").replace(\".safetensors\", \"{suffix}.safetensors\")\n",
      "    state_dict_split = split_torch_state_dict_into_shards(\n",
      "        llama_state_dict, filename_pattern=filename_pattern, max_shard_size=shard_size\n",
      "    )\n",
      "    for shard_file, tensors in tqdm(state_dict_split.filename_to_tensors.items(), desc=\"Save weights\"):\n",
      "        shard = {tensor: llama_state_dict[tensor].contiguous() for tensor in tensors}\n",
      "        if save_safetensors:\n",
      "            save_file(shard, os.path.join(output_dir, shard_file), metadata={\"format\": \"pt\"})\n",
      "        else:\n",
      "            torch.save(shard, os.path.join(output_dir, shard_file))\n",
      "\n",
      "    if not state_dict_split.is_sharded:\n",
      "        print(f\"Model weights saved in {os.path.join(output_dir, weights_name)}.\")\n",
      "    else:\n",
      "        index = {\n",
      "            \"metadata\": state_dict_split.metadata,\n",
      "            \"weight_map\": state_dict_split.tensor_to_filename,\n",
      "        }\n",
      "        index_name = SAFE_WEIGHTS_INDEX_NAME if save_safetensors else WEIGHTS_INDEX_NAME\n",
      "        with open(os.path.join(output_dir, index_name), \"w\", encoding=\"utf-8\") as f:\n",
      "            json.dump(index, f, indent=2, sort_keys=True)\n",
      "\n",
      "        print(f\"Model weights saved in {output_dir}.\")\n",
      "\n",
      "    return str(torch_dtype).replace(\"torch.\", \"\")\n",
      "\n",
      "\n",
      "def save_config(input_dir: str, output_dir: str, torch_dtype: str):\n",
      "    with open(os.path.join(input_dir, CONFIG_NAME), encoding=\"utf-8\") as f:\n",
      "        qwen_config_dict: Dict[str, Any] = json.load(f)\n",
      "\n",
      "    llama2_config_dict: Dict[str, Any] = OrderedDict()\n",
      "    llama2_config_dict[\"architectures\"] = [\"LlamaForCausalLM\"]\n",
      "    llama2_config_dict[\"hidden_act\"] = \"silu\"\n",
      "    llama2_config_dict[\"hidden_size\"] = qwen_config_dict[\"hidden_size\"]\n",
      "    llama2_config_dict[\"initializer_range\"] = qwen_config_dict[\"initializer_range\"]\n",
      "    llama2_config_dict[\"intermediate_size\"] = qwen_config_dict[\"intermediate_size\"] // 2\n",
      "    llama2_config_dict[\"max_position_embeddings\"] = qwen_config_dict[\"max_position_embeddings\"]\n",
      "    llama2_config_dict[\"model_type\"] = \"llama\"\n",
      "    llama2_config_dict[\"num_attention_heads\"] = qwen_config_dict[\"num_attention_heads\"]\n",
      "    llama2_config_dict[\"num_hidden_layers\"] = qwen_config_dict[\"num_hidden_layers\"]\n",
      "    llama2_config_dict[\"num_key_value_heads\"] = qwen_config_dict[\"hidden_size\"] // qwen_config_dict[\"kv_channels\"]\n",
      "    llama2_config_dict[\"pretraining_tp\"] = 1\n",
      "    llama2_config_dict[\"rms_norm_eps\"] = qwen_config_dict[\"layer_norm_epsilon\"]\n",
      "    llama2_config_dict[\"rope_scaling\"] = None\n",
      "    llama2_config_dict[\"tie_word_embeddings\"] = qwen_config_dict[\"tie_word_embeddings\"]\n",
      "    llama2_config_dict[\"torch_dtype\"] = torch_dtype\n",
      "    llama2_config_dict[\"transformers_version\"] = \"4.34.0\"\n",
      "    llama2_config_dict[\"use_cache\"] = True\n",
      "    llama2_config_dict[\"vocab_size\"] = qwen_config_dict[\"vocab_size\"]\n",
      "    llama2_config_dict[\"attention_bias\"] = True\n",
      "\n",
      "    with open(os.path.join(output_dir, CONFIG_NAME), \"w\", encoding=\"utf-8\") as f:\n",
      "        json.dump(llama2_config_dict, f, indent=2)\n",
      "\n",
      "    print(f\"Model config saved in {os.path.join(output_dir, CONFIG_NAME)}\")\n",
      "\n",
      "\n",
      "def llamafy_qwen(\n",
      "    input_dir: str,\n",
      "    output_dir: str,\n",
      "    shard_size: str = \"2GB\",\n",
      "    save_safetensors: bool = False,\n",
      "):\n",
      "    r\"\"\"\n",
      "    Converts the Qwen models in the same format as LLaMA2.\n",
      "    Usage: python llamafy_qwen.py --input_dir input --output_dir output\n",
      "    Converted model: https://huggingface.co/hiyouga/Qwen-14B-Chat-LLaMAfied\n",
      "    \"\"\"\n",
      "    try:\n",
      "        os.makedirs(output_dir, exist_ok=False)\n",
      "    except Exception as e:\n",
      "        raise print(\"Output dir already exists\", e)\n",
      "\n",
      "    torch_dtype = save_weight(input_dir, output_dir, shard_size, save_safetensors)\n",
      "    save_config(input_dir, output_dir, torch_dtype)\n",
      "\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    fire.Fire(llamafy_qwen)\n",
      "\n",
      "```</s>\n"
     ]
    }
   ],
   "source": [
    "for row in dataset[:5][\"text\"]:\n",
    "    print(\"=========================\")\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/unslothai/unsloth.git\n",
      "  Cloning https://github.com/unslothai/unsloth.git to /tmp/pip-req-build-9g9s0zfx\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/unslothai/unsloth.git /tmp/pip-req-build-9g9s0zfx\n",
      "  Resolved https://github.com/unslothai/unsloth.git to commit fe04c014c22616cba56c9ddf7782be8ea8ed117e\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hBuilding wheels for collected packages: unsloth\n",
      "  Building wheel for unsloth (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for unsloth: filename=unsloth-2025.3.10-py3-none-any.whl size=192773 sha256=b4b72d1b9c8b11cd35a503e58e8f7a69cfedcf3150fd70b7717aa51f992156a5\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-h1ox41gm/wheels/d1/17/05/850ab10c33284a4763b0595cd8ea9d01fce6e221cac24b3c01\n",
      "Successfully built unsloth\n",
      "Installing collected packages: unsloth\n",
      "  Attempting uninstall: unsloth\n",
      "    Found existing installation: unsloth 2025.3.10\n",
      "    Uninstalling unsloth-2025.3.10:\n",
      "      Successfully uninstalled unsloth-2025.3.10\n",
      "Successfully installed unsloth-2025.3.10\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade --force-reinstall --no-cache-dir git+https://github.com/unslothai/unsloth.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.chat_template = \"{% for message in messages %}{{'<s>' + message['role'] + '\\n' + message['content'] + '</s>' + '\\n'}}{% endfor %}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "idAEIeSQ3xdS"
   },
   "source": [
    "<a name=\"Train\"></a>\n",
    "### Continued Pretraining\n",
    "Now let's use Unsloth's `UnslothTrainer`! More docs here: [TRL SFT docs](https://huggingface.co/docs/trl/sft_trainer). We do 20 steps to speed things up, but you can set `num_train_epochs=1` for a full run, and turn off `max_steps=None`.\n",
    "\n",
    "Also set `embedding_learning_rate` to be a learning rate at least 2x or 10x smaller than `learning_rate` to make continual pretraining work!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "10fc22494a3b47cb8927f60261f15c2e",
      "548de612a42240e7a742ae47043ed3d6",
      "e8fd479dae84465bbc33d83a4ef86a5b",
      "569e08c2b94d43a9aaf5b3699292d90e",
      "bfc70a46ac154abaaf49cae2f1a6d402",
      "247c4e1c106544eea292866aa2755ff5",
      "7c23d0d1612b49b9bd5b21c4f5c8719b",
      "b4b3722e5e154622bd5fe4978b3946ef",
      "894c2e23ceb24c928854266cd0701ed6",
      "175dba6b0d9242e0b4a1b2d43eee83ab",
      "fe37666964354bec9767001e6e561217"
     ]
    },
    "id": "95_Nn-89DhsL",
    "outputId": "adbeec8d-8580-4b4a-9b2b-96dcaf8fe66a"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b949ebe03acc43c982c401fee0c93c4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Unsloth: Tokenizing [\"text\"] (num_proc=8):   0%|          | 0/158 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "from unsloth import is_bfloat16_supported\n",
    "from unsloth import UnslothTrainer, UnslothTrainingArguments\n",
    "\n",
    "trainer = UnslothTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = dataset,\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dataset_num_proc = 8,\n",
    "\n",
    "    args = UnslothTrainingArguments(\n",
    "        per_device_train_batch_size = 2,\n",
    "        gradient_accumulation_steps = 8,\n",
    "\n",
    "        warmup_ratio = 0.1,\n",
    "        num_train_epochs = 2,\n",
    "\n",
    "        learning_rate = 5e-5,\n",
    "        embedding_learning_rate = 5e-6,\n",
    "\n",
    "        fp16 = not is_bfloat16_supported(),\n",
    "        bf16 = is_bfloat16_supported(),\n",
    "        logging_steps = 1,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.00,\n",
    "        lr_scheduler_type = \"cosine\",\n",
    "        seed = 3407,\n",
    "        output_dir = \"outputs\",\n",
    "        report_to = \"none\", # Use this for WandB etc\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2ejIt2xSNKKp",
    "outputId": "808e5a29-ee68-41fb-e736-29366e6d3369"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU = NVIDIA L40S. Max memory = 44.521 GB.\n",
      "43.029 GB of memory reserved.\n"
     ]
    }
   ],
   "source": [
    "#@title Show current memory stats\n",
    "gpu_stats = torch.cuda.get_device_properties(0)\n",
    "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
    "print(f\"{start_gpu_memory} GB of memory reserved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "yqxqAZ7KJ4oL",
    "outputId": "360177e1-a92c-4f35-f3ae-7c60e166b0ea"
   },
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 1.97 GiB. GPU 0 has a total capacity of 44.52 GiB of which 1010.25 MiB is free. Process 3509446 has 43.53 GiB memory in use. Of the allocated memory 43.01 GiB is allocated by PyTorch, and 22.45 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m trainer_stats \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py:2180\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2177\u001b[0m \u001b[38;5;66;03m# do_train is not a reliable argument, as it might not be set and .train() still called, so\u001b[39;00m\n\u001b[1;32m   2178\u001b[0m \u001b[38;5;66;03m# the following is a workaround:\u001b[39;00m\n\u001b[1;32m   2179\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (args\u001b[38;5;241m.\u001b[39mfp16_full_eval \u001b[38;5;129;01mor\u001b[39;00m args\u001b[38;5;241m.\u001b[39mbf16_full_eval) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m args\u001b[38;5;241m.\u001b[39mdo_train \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_model_parallel:\n\u001b[0;32m-> 2180\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_move_model_to_device\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_path\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m kwargs:\n\u001b[1;32m   2183\u001b[0m     resume_from_checkpoint \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_path\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py:899\u001b[0m, in \u001b[0;36mTrainer._move_model_to_device\u001b[0;34m(self, model, device)\u001b[0m\n\u001b[1;32m    898\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_move_model_to_device\u001b[39m(\u001b[38;5;28mself\u001b[39m, model, device):\n\u001b[0;32m--> 899\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    900\u001b[0m     \u001b[38;5;66;03m# Moving a model to an XLA device disconnects the tied weights, so we have to retie them.\u001b[39;00m\n\u001b[1;32m    901\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mparallel_mode \u001b[38;5;241m==\u001b[39m ParallelMode\u001b[38;5;241m.\u001b[39mTPU \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(model, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtie_weights\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1343\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1340\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1341\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m-> 1343\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:903\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    901\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    902\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 903\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    905\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    906\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    907\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    908\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    913\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    914\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:903\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    901\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    902\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 903\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    905\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    906\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    907\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    908\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    913\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    914\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "    \u001b[0;31m[... skipping similar frames: Module._apply at line 903 (2 times)]\u001b[0m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:903\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    901\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    902\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 903\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    905\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    906\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    907\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    908\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    913\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    914\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:930\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    926\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    927\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    928\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    929\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 930\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    931\u001b[0m p_should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    933\u001b[0m \u001b[38;5;66;03m# subclasses may have multiple child tensors so we need to use swap_tensors\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1329\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1322\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m   1323\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(\n\u001b[1;32m   1324\u001b[0m             device,\n\u001b[1;32m   1325\u001b[0m             dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1326\u001b[0m             non_blocking,\n\u001b[1;32m   1327\u001b[0m             memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format,\n\u001b[1;32m   1328\u001b[0m         )\n\u001b[0;32m-> 1329\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1330\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1331\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1332\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1333\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1334\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1335\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot copy out of meta tensor; no data!\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 1.97 GiB. GPU 0 has a total capacity of 44.52 GiB of which 1010.25 MiB is free. Process 3509446 has 43.53 GiB memory in use. Of the allocated memory 43.01 GiB is allocated by PyTorch, and 22.45 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pCqnaKmlO1U9",
    "outputId": "5c282a92-ce43-426a-d92a-1f481ac86209"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "527.2059 seconds used for training.\n",
      "8.79 minutes used for training.\n",
      "Peak reserved memory = 11.695 GB.\n",
      "Peak reserved memory for training = 5.816 GB.\n",
      "Peak reserved memory % of max memory = 79.337 %.\n",
      "Peak reserved memory for training % of max memory = 39.455 %.\n"
     ]
    }
   ],
   "source": [
    "#@title Show final memory and time stats\n",
    "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
    "used_percentage = round(used_memory         /max_memory*100, 3)\n",
    "lora_percentage = round(used_memory_for_lora/max_memory*100, 3)\n",
    "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
    "print(f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\")\n",
    "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
    "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
    "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
    "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ekOmTR1hSNcr"
   },
   "source": [
    "<a name=\"Inference\"></a>\n",
    "### Inference\n",
    "Let's run the model!\n",
    "\n",
    "We first will try to see if the model follows the style and understands to write a story that is within the distribution of \"Tiny Stories\". Ie a story fit for a bed time story most likely.\n",
    "\n",
    "We select \"Once upon a time, in a galaxy, far far away,\" since it normally is associated with Star Wars."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "RHm11vaQRt1U",
    "outputId": "e633259d-3a83-437d-c009-d97b0146aab6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> class"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-24 (unsloth_fast_generate):\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.11/threading.py\", line 1045, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.11/threading.py\", line 982, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/unsloth/models/llama.py\", line 1578, in unsloth_fast_generate\n",
      "    output = self._old_generate(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/peft/peft_model.py\", line 1838, in generate\n",
      "    outputs = self.base_model.generate(*args, **kwargs)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/unsloth/models/llama.py\", line 1578, in unsloth_fast_generate\n",
      "    output = self._old_generate(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py\", line 2255, in generate\n",
      "    result = self._sample(\n",
      "             ^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py\", line 3257, in _sample\n",
      "    outputs = model_forward(**model_inputs, return_dict=True)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/unsloth/models/mistral.py\", line 214, in MistralForCausalLM_fast_forward\n",
      "    outputs = LlamaModel_fast_forward_inference(\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/unsloth/models/llama.py\", line 961, in LlamaModel_fast_forward_inference\n",
      "    X, present_key_value = LlamaAttention_fast_forward_inference(\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/unsloth/models/llama.py\", line 235, in LlamaAttention_fast_forward_inference\n",
      "    self.paged_attention_K[seq_len] = Kn.permute(2, 0, 1, 3)\n",
      "    ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1928, in __getattr__\n",
      "    raise AttributeError(\n",
      "AttributeError: 'MistralAttention' object has no attribute 'paged_attention_K'\n",
      "Exception in thread Thread-25 (unsloth_fast_generate):\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.11/threading.py\", line 1045, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.11/threading.py\", line 982, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/unsloth/models/llama.py\", line 1578, in unsloth_fast_generate\n",
      "    output = self._old_generate(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/peft/peft_model.py\", line 1838, in generate\n",
      "    outputs = self.base_model.generate(*args, **kwargs)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/unsloth/models/llama.py\", line 1578, in unsloth_fast_generate\n",
      "    output = self._old_generate(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py\", line 2255, in generate\n",
      "    result = self._sample(\n",
      "             ^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py\", line 3257, in _sample\n",
      "    outputs = model_forward(**model_inputs, return_dict=True)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/unsloth/models/mistral.py\", line 214, in MistralForCausalLM_fast_forward\n",
      "    outputs = LlamaModel_fast_forward_inference(\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/unsloth/models/llama.py\", line 961, in LlamaModel_fast_forward_inference\n",
      "    X, present_key_value = LlamaAttention_fast_forward_inference(\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/unsloth/models/llama.py\", line 202, in LlamaAttention_fast_forward_inference\n",
      "    Vn = fast_linear_forward(self.v_proj, Xn, out = self.temp_KV[1])\n",
      "                                                    ^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1928, in __getattr__\n",
      "    raise AttributeError(\n",
      "AttributeError: 'MistralAttention' object has no attribute 'temp_KV'. Did you mean: 'temp_O'?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ScoreEvaluationResponse(BaseModel):\n",
      "   r\"\"\"\n",
      "   Copyright 2024 the HuggingFace team.\n",
      "\n",
      "   \n",
      "This code is inspired by the HuggingFace's transformers library.\n",
      "   \n",
      "https://github.com/huggingface/transformers/blob/v4.40.0/examples/pytorch/language-modeling/run_clm.py\n",
      "   \"\"\"\n",
      "   from typing import TYPE_CHECKING, Dict, List, Optional, Sequence, Tuple, Union\n",
      "\n",
      "   from \n",
      "transformers import DataCollatorWithPadding, EvalPrediction, ProcessorMixin, TrainerCallback, TrainerState\n",
      "   \n",
      "from transformers.trainer import is_main_process\n",
      "\n",
      "   from ..extras import logging\n",
      "   from \n",
      "..extras.constants import IGNORE_INDEX\n",
      "   from ..extras.misc import is_torch_version_greater_than\n",
      "   from \n",
      "..extras.utils import is_data_collator_available\n",
      "   from ..extras.version import \n",
      "is_transformers_version_greater_than\n",
      "\n",
      "   if is_data_collator_available():\n",
      "       from transformers.data import \n",
      "DataCollatorWithPaddingForSeq2Seq\n",
      "\n",
      "   if is_torch_version_greater_than(\"1.14.0\"):\n",
      "       from transformers.trainer import \n",
      "get_current_device\n",
      "\n",
      "   if is_transformers_version_greater_than(\"4.40.0\"):\n",
      "       from transformers.trainer import \n",
      "get_current_device\n",
      "\n",
      "   if TYPE_CHECKING:\n",
      "       from transformers import DataCollatorWithPaddingForSeq2Seq, \n",
      "ProcessorMixin, TrainerCallback, TrainerState\n",
      "\n",
      "\n",
      "def create_model_and_tokenizer(\n",
      "   model_name_or_path: str,\n",
      "   \n",
      "model_args: Optional[Dict[str, Union[str, int, float]]] = None,\n",
      "   tokenizer_name: Optional[str] = None,\n",
      "   \n",
      "tokenizer_args: Optional[Dict[str, Union[str, int, float]]] = None,\n",
      "   **kwargs,\n",
      ") "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-47a6f8f6ef23>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0mlength\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_text\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_streamer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mj\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mwrapped_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtextwrap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwidth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax_print_width\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/generation/streamers.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    224\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 226\u001b[0;31m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    227\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_signal\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.11/queue.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    169\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m                 \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_qsize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnot_empty\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    172\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"'timeout' must be a non-negative number\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.11/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    325\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 327\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    328\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from transformers import TextIteratorStreamer\n",
    "from threading import Thread\n",
    "text_streamer = TextIteratorStreamer(tokenizer)\n",
    "import textwrap\n",
    "max_print_width = 100\n",
    "\n",
    "inputs = tokenizer(\n",
    "[\n",
    "    \"class ScoreEvaluationResponse(BaseModel):\"\n",
    "]*1, return_tensors = \"pt\").to(\"cuda\")\n",
    "\n",
    "generation_kwargs = dict(\n",
    "    inputs,\n",
    "    streamer = text_streamer,\n",
    "    max_new_tokens = 2000,\n",
    "    use_cache = True,\n",
    ")\n",
    "thread = Thread(target = model.generate, kwargs = generation_kwargs)\n",
    "thread.start()\n",
    "\n",
    "length = 0\n",
    "for j, new_text in enumerate(text_streamer):\n",
    "    if j == 0:\n",
    "        wrapped_text = textwrap.wrap(new_text, width = max_print_width)\n",
    "        length = len(wrapped_text[-1])\n",
    "        wrapped_text = \"\\n\".join(wrapped_text)\n",
    "        print(wrapped_text, end = \"\")\n",
    "    else:\n",
    "        length += len(new_text)\n",
    "        if length >= max_print_width:\n",
    "            length = 0\n",
    "            print()\n",
    "        print(new_text, end = \"\")\n",
    "    pass\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zt9CHJqO6p30"
   },
   "source": [
    "And we're done! If you have any questions on Unsloth, we have a [Discord](https://discord.gg/u54VK8m8tk) channel! If you find any bugs or want to keep updated with the latest LLM stuff, or need help, join projects etc, feel free to join our Discord!\n",
    "\n",
    "Some other links:\n",
    "1. Zephyr DPO 2x faster [free Colab](https://colab.research.google.com/drive/15vttTpzzVXv_tJwEk-hIcQ0S9FcEWvwP?usp=sharing)\n",
    "2. Llama 7b 2x faster [free Colab](https://colab.research.google.com/drive/1lBzz5KeZJKXjvivbYvmGarix9Ao6Wxe5?usp=sharing)\n",
    "3. TinyLlama 4x faster full Alpaca 52K in 1 hour [free Colab](https://colab.research.google.com/drive/1AZghoNBQaMDgWJpi4RbffGM1h6raLUj9?usp=sharing)\n",
    "4. CodeLlama 34b 2x faster [A100 on Colab](https://colab.research.google.com/drive/1y7A0AxE3y8gdj4AVkl2aZX47Xu3P1wJT?usp=sharing)\n",
    "5. Mistral 7b [free Kaggle version](https://www.kaggle.com/code/danielhanchen/kaggle-mistral-7b-unsloth-notebook)\n",
    "6. We also did a [blog](https://huggingface.co/blog/unsloth-trl) with ü§ó HuggingFace, and we're in the TRL [docs](https://huggingface.co/docs/trl/main/en/sft_trainer#accelerate-fine-tuning-2x-using-unsloth)!\n",
    "7. `ChatML` for ShareGPT datasets, [conversational notebook](https://colab.research.google.com/drive/1Aau3lgPzeZKQ-98h69CCu1UJcvIBLmy2?usp=sharing)\n",
    "8. Gemma 6 trillion tokens is 2.5x faster! [free Colab](https://colab.research.google.com/drive/10NbwlsRChbma1v55m8LAPYG15uQv6HLo?usp=sharing)\n",
    "\n",
    "<div class=\"align-center\">\n",
    "  <a href=\"https://github.com/unslothai/unsloth\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/unsloth%20new%20logo.png\" width=\"115\"></a>\n",
    "  <a href=\"https://discord.gg/u54VK8m8tk\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/Discord.png\" width=\"145\"></a>\n",
    "  <a href=\"https://ko-fi.com/unsloth\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/Kofi button.png\" width=\"145\"></a></a> Support our work if you can! Thanks!\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "000f8c3e543e474b906ae3ed88e3029b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "00e8bc9a30dd46d0ab5bd64d8cb603c9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "014eb62c344349119bfdd746d235719e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2f63f930dd9046a1a6ca4e161091996d",
      "max": 446,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_3b150c7d44a845ebad1c1f28cef3ac26",
      "value": 446
     }
    },
    "0290aa5b6f2148c28d5db378da4eaa9d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "danger",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_000f8c3e543e474b906ae3ed88e3029b",
      "max": 4138270821,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_b07aa794ee6246ac89b923fa3bfbc4f3",
      "value": 4138270427
     }
    },
    "0b235b37c13e4c8197c8fd149ed49a28": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "0b4bb49b21094acd9cf0f698a1b5dc9f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0e6a85d4230b4d868eab609e3bbcb26b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "10fc22494a3b47cb8927f60261f15c2e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_548de612a42240e7a742ae47043ed3d6",
       "IPY_MODEL_e8fd479dae84465bbc33d83a4ef86a5b",
       "IPY_MODEL_569e08c2b94d43a9aaf5b3699292d90e"
      ],
      "layout": "IPY_MODEL_bfc70a46ac154abaaf49cae2f1a6d402"
     }
    },
    "12a3eb9a7ad840c1a04b389fb848855b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4e804623e6ac44d099a7aa0de89aa300",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_f24328d31e3d48d0bd76785140125a28",
      "value": "tokenizer.json:‚Äá100%"
     }
    },
    "13628eb9c7864898a4c95b614fc8f3cb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_20a318008c5f4dd083b0cedbd347d604",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_5019072f0df344828bf62842e0d70aae",
      "value": "‚Äá446/446‚Äá[00:00&lt;00:00,‚Äá43.3kB/s]"
     }
    },
    "175dba6b0d9242e0b4a1b2d43eee83ab": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "200d56e0e3e94909bba8ca04777d1769": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "20a318008c5f4dd083b0cedbd347d604": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "213242e14f81470a943f3d3608223907": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "247c4e1c106544eea292866aa2755ff5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "286282f0f5f14ed7b5516a5c86a14717": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5f7b89d1f7bd4c04a1be54d62db1de90",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_3d385dbd0f9742cbba14a00e00238c7a",
      "value": "model.safetensors:‚Äá100%"
     }
    },
    "2f63f930dd9046a1a6ca4e161091996d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "38b260d5f1604e3ab8303bdc7bed09a2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0b4bb49b21094acd9cf0f698a1b5dc9f",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_78c4e03ba858481eb017abbad634e563",
      "value": "‚Äá157/157‚Äá[00:00&lt;00:00,‚Äá4.48kB/s]"
     }
    },
    "3b150c7d44a845ebad1c1f28cef3ac26": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "3d385dbd0f9742cbba14a00e00238c7a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "45ef29a0d84641e5a34eca5d97f285b9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6d6ab6642f074af7ad3c37e10dd7de12",
      "max": 587404,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_213242e14f81470a943f3d3608223907",
      "value": 587404
     }
    },
    "4a4ebf72168c487bbe2138ad295211e2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "4e804623e6ac44d099a7aa0de89aa300": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4f8e75200153430892f20a2e0d9683ad": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5019072f0df344828bf62842e0d70aae": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "5206db6138cb4d3486729940f430193b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "548de612a42240e7a742ae47043ed3d6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_247c4e1c106544eea292866aa2755ff5",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_7c23d0d1612b49b9bd5b21c4f5c8719b",
      "value": "Tokenizing‚Äáto‚Äá[&quot;text&quot;]‚Äá(num_proc=8):‚Äá100%"
     }
    },
    "569e08c2b94d43a9aaf5b3699292d90e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_175dba6b0d9242e0b4a1b2d43eee83ab",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_fe37666964354bec9767001e6e561217",
      "value": "‚Äá79/79‚Äá[00:03&lt;00:00,‚Äá53.48‚Äáexamples/s]"
     }
    },
    "593b56fd26cc4a91948934b9881d37c3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "5b7e9400c0584cb3b8556ced0f9fb024": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "5c0aef5eaef84c6a885185551a99f489": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5c88f1449b204e3ca33c7c0fdca5a2e3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "5f216daf476d4ffaabccdc97deab3802": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4f8e75200153430892f20a2e0d9683ad",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_b7c8be1947f948dcbe988f11e2a47280",
      "value": "‚Äá4.14G/4.14G‚Äá[00:38&lt;00:00,‚Äá107MB/s]"
     }
    },
    "5f7b89d1f7bd4c04a1be54d62db1de90": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5fb7c2fe970543a2affb765ef1e489e3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "693260784e824a90b2931c520749963a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6d6ab6642f074af7ad3c37e10dd7de12": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "738bc2d947da4c9f93eaec6dc10865ea": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "75c6f712315f4d448506cc642f852bad": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "7839c152c70f4333b6fd56a0de422ed8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_ac531852a2e0473e8af8ba9962fe4b91",
       "IPY_MODEL_f88f3dc016c94bbc81f9c296693f24dd",
       "IPY_MODEL_8be71e7b5e914b0e8a22eddef64a4a94"
      ],
      "layout": "IPY_MODEL_9ce88525df584216aace0b168e4a4798"
     }
    },
    "78c4e03ba858481eb017abbad634e563": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "7a6443281ed24a2a8d34d0460077e02a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_12a3eb9a7ad840c1a04b389fb848855b",
       "IPY_MODEL_b8183970a8114c85b0f292b2c6c9e123",
       "IPY_MODEL_c88ef51daaf0412fac372cbb5b722335"
      ],
      "layout": "IPY_MODEL_e93e414082b64fffa09df3091dce6f72"
     }
    },
    "7aba3c7d80b64710a8861f6a065daade": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7c23d0d1612b49b9bd5b21c4f5c8719b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "7d11bfead4744ce7abff3ee28aebd9c5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_db64af76fabf4e4686e0e70a668bc112",
       "IPY_MODEL_45ef29a0d84641e5a34eca5d97f285b9",
       "IPY_MODEL_c63635c3ee9d42bb8b5214ceb05f58f6"
      ],
      "layout": "IPY_MODEL_738bc2d947da4c9f93eaec6dc10865ea"
     }
    },
    "894c2e23ceb24c928854266cd0701ed6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "8be71e7b5e914b0e8a22eddef64a4a94": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_acfb3d307dd24dac957e4fe9efcf0604",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_5c88f1449b204e3ca33c7c0fdca5a2e3",
      "value": "‚Äá137k/137k‚Äá[00:00&lt;00:00,‚Äá3.47MB/s]"
     }
    },
    "998156a97ea54b47a144feb168b422f3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "9a08155d3e164f4abedbea66efda5d76": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "9c86fec49106407e8cdb0728bf8283d5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9ce88525df584216aace0b168e4a4798": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a9c82ef6d857430ca0ed5a8d06afc5b9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a9e959ffd8f242b0be7cd48285aee3f8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ac531852a2e0473e8af8ba9962fe4b91": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7aba3c7d80b64710a8861f6a065daade",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_5b7e9400c0584cb3b8556ced0f9fb024",
      "value": "tokenizer_config.json:‚Äá100%"
     }
    },
    "acfb3d307dd24dac957e4fe9efcf0604": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b07aa794ee6246ac89b923fa3bfbc4f3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "b4a9e32a271a46438e9c188d311183c3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5c0aef5eaef84c6a885185551a99f489",
      "max": 157,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_593b56fd26cc4a91948934b9881d37c3",
      "value": 157
     }
    },
    "b4b3722e5e154622bd5fe4978b3946ef": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b7c8be1947f948dcbe988f11e2a47280": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "b8183970a8114c85b0f292b2c6c9e123": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_693260784e824a90b2931c520749963a",
      "max": 1961548,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_e3bd04fcfd734ac8b6ee5af8b0d1131b",
      "value": 1961548
     }
    },
    "bfc70a46ac154abaaf49cae2f1a6d402": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c63635c3ee9d42bb8b5214ceb05f58f6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a9e959ffd8f242b0be7cd48285aee3f8",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_0b235b37c13e4c8197c8fd149ed49a28",
      "value": "‚Äá587k/587k‚Äá[00:00&lt;00:00,‚Äá7.89MB/s]"
     }
    },
    "c88ef51daaf0412fac372cbb5b722335": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_200d56e0e3e94909bba8ca04777d1769",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_9a08155d3e164f4abedbea66efda5d76",
      "value": "‚Äá1.96M/1.96M‚Äá[00:00&lt;00:00,‚Äá14.6MB/s]"
     }
    },
    "d328c3949a9e4092883d711d7fb9d945": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d94e0126736f4f7bbdc350f2b25c57a8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0e6a85d4230b4d868eab609e3bbcb26b",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_998156a97ea54b47a144feb168b422f3",
      "value": "generation_config.json:‚Äá100%"
     }
    },
    "db64af76fabf4e4686e0e70a668bc112": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5fb7c2fe970543a2affb765ef1e489e3",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_4a4ebf72168c487bbe2138ad295211e2",
      "value": "tokenizer.model:‚Äá100%"
     }
    },
    "e0018988debb4ccabcbdae7b2f2fe273": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5206db6138cb4d3486729940f430193b",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_75c6f712315f4d448506cc642f852bad",
      "value": "special_tokens_map.json:‚Äá100%"
     }
    },
    "e3bd04fcfd734ac8b6ee5af8b0d1131b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "e493a78dc35b4e989bcf8412efd992f1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_286282f0f5f14ed7b5516a5c86a14717",
       "IPY_MODEL_0290aa5b6f2148c28d5db378da4eaa9d",
       "IPY_MODEL_5f216daf476d4ffaabccdc97deab3802"
      ],
      "layout": "IPY_MODEL_d328c3949a9e4092883d711d7fb9d945"
     }
    },
    "e8aec73257844a19bcb404ea22a09d2c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_d94e0126736f4f7bbdc350f2b25c57a8",
       "IPY_MODEL_b4a9e32a271a46438e9c188d311183c3",
       "IPY_MODEL_38b260d5f1604e3ab8303bdc7bed09a2"
      ],
      "layout": "IPY_MODEL_00e8bc9a30dd46d0ab5bd64d8cb603c9"
     }
    },
    "e8fd479dae84465bbc33d83a4ef86a5b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b4b3722e5e154622bd5fe4978b3946ef",
      "max": 79,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_894c2e23ceb24c928854266cd0701ed6",
      "value": 79
     }
    },
    "e92d928ff17e466fbf0b3934f987cdaa": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "e93e414082b64fffa09df3091dce6f72": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f24328d31e3d48d0bd76785140125a28": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f88f3dc016c94bbc81f9c296693f24dd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9c86fec49106407e8cdb0728bf8283d5",
      "max": 136734,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_e92d928ff17e466fbf0b3934f987cdaa",
      "value": 136734
     }
    },
    "fdbe574155984ae689e858c45ce529cf": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_e0018988debb4ccabcbdae7b2f2fe273",
       "IPY_MODEL_014eb62c344349119bfdd746d235719e",
       "IPY_MODEL_13628eb9c7864898a4c95b614fc8f3cb"
      ],
      "layout": "IPY_MODEL_a9c82ef6d857430ca0ed5a8d06afc5b9"
     }
    },
    "fe37666964354bec9767001e6e561217": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
